#!/usr/bin/env python
"""
This is the main program to run Mindboggle.

For help in using Mindboggle ::

    - Online `documentation <http://mindboggle.info/documentation.html>`_
    - README file
    - Help on the command line::

        $ mindboggle --help

This file uses Nipype (http://www.nipy.org/nipype/) to create a workflow
environment to enable Mindboggle to run in a flexible, modular manner
while storing provenance information.

Authors:
    - Arno Klein, 2010-2014  (arno@mindboggle.info)  http://binarybottle.com
    - Satrajit S. Ghosh, 2013  (satra@mit.edu)  http://www.mit.edu/~satra/
    - Each file lists Mindboggle team members who contributed to its content.

Copyright 2014,  Mindboggle team (http://mindboggle.info), Apache v2.0 License

"""

#=============================================================================
#
#   Import libraries
#
#=============================================================================
import os
import argparse
import warnings
#-----------------------------------------------------------------------------
# Nipype libraries
#-----------------------------------------------------------------------------
from nipype import config, logging
from nipype.pipeline.engine import Workflow, Node, JoinNode
from nipype.interfaces.io import DataGrabber, DataSink
from nipype.interfaces.utility import Function as Fn
from nipype.interfaces.utility import IdentityInterface
from nipype.interfaces.ants import ApplyTransforms
warnings.filterwarnings("ignore")
#-----------------------------------------------------------------------------
# Mindboggle libraries
#-----------------------------------------------------------------------------
from mindboggle.LABELS import DKTprotocol
from mindboggle.DATA import hashes_url
from mindboggle.evaluate.evaluate_labels import measure_surface_overlap, \
    measure_volume_overlap
from mindboggle.features.folds import extract_folds
from mindboggle.features.fundi import extract_fundi, segment_fundi
from mindboggle.features.sulci import extract_sulci
from mindboggle.shapes.laplace_beltrami import spectrum_per_label
from mindboggle.shapes.likelihood import compute_likelihood
from mindboggle.labels.relabel import relabel_surface, relabel_volume, \
    keep_volume_labels, remove_volume_labels, overwrite_volume_labels
from mindboggle.shapes.shape_tools import area, travel_depth, \
    geodesic_depth, curvature
from mindboggle.shapes.zernike.zernike import zernike_moments_per_label
from mindboggle.utils.ants import fetch_ants_data, ComposeMultiTransform, \
    ImageMath, PropagateLabelsThroughMask, fill_volume_with_surface_labels, \
    thickinthehead
from mindboggle.utils.compute import volume_per_label
from mindboggle.utils.freesurfer import surface_to_vtk, curvature_to_vtk, \
    annot_to_vtk, label_with_classifier, convert_mgh_to_native_nifti
from mindboggle.utils.io_table import write_columns, \
    write_shape_stats, write_vertex_measures
from mindboggle.utils.io_uri import retrieve_data
from mindboggle.utils.io_vtk import read_vtk, apply_affine_transform
from mindboggle.utils.mesh import rescale_by_neighborhood
from mindboggle.utils.paths import smooth_skeleton
from mindboggle.utils.segment import split_brain, combine_2labels_in_2volumes
from mindboggle.utils.utils import list_strings

#=============================================================================
#
#   Command-line arguments
#
#=============================================================================
parser = argparse.ArgumentParser(description="""
                    The Mindboggle software automates shape analysis of
                    anatomical labels and features extracted from human brain
                    MR image data. For more information, please see
                    http://mindboggle.info/users/README.html""",
                     formatter_class = lambda prog:
                     argparse.HelpFormatter(prog, max_help_position=40))

ants_group = parser.add_argument_group('ANTs inputs')
features_group = parser.add_argument_group('more features')
shapes_group = parser.add_argument_group('more shapes')
inputs_group = parser.add_argument_group('modify inputs')
outputs_group = parser.add_argument_group('modify outputs')
extras_group = parser.add_argument_group('extras')

parser.add_argument("SUBJECT",
                    help=('Subject name corresponding to the name of a '
                          'directory in the FreeSurfer $SUBJECTS_DIR directory'))
parser.add_argument("-v", "--version", help="version number",
                    action='version', version='%(prog)s 0.1')
parser.add_argument("-n",
                    help=('number of processors (default: 1)'),
                    type=int, default=1, metavar='INT')

ants_group.add_argument("--ants_segments",
                    help=("Segmented file from antsCorticalThickness.sh"),
                    metavar='STR')

features_group.add_argument("--sulci", action='store_true',
                    help="Extract, identify, and measure sulci")
features_group.add_argument("--fundi", action='store_true',
                    help="Extract, identify, and measure fundi "
                         "[UNTESTED]")
features_group.add_argument("--vertices", action='store_true',
                    help=("Make table of per-vertex surface shape measures"))

shapes_group.add_argument("--thickness", action='store_true',
                    help="Compute cortical label thicknesses with ANTs")
shapes_group.add_argument("--spectra",
                    help='Number of Laplace-Beltrami spectrum eigenvalues '
                         'per label/feature '
                         '(recommended: 10; default OFF)',
                    default=0, type=int, metavar='INT')
shapes_group.add_argument("--moments",
                    help='Order of Zernike moments per label/feature '
                         '(recommended but SLOW: 10; default OFF)',
                    default=0, type=int, metavar='INT')

inputs_group.add_argument("--subjects_dir",
                    help=("FreeSurfer subjects directory (default: "
                          "$SUBJECTS_DIR)"),
                    metavar='STR')
inputs_group.add_argument("--volume_labels",
                    help=("Volume labels: \"wmparc\" (default) or "
                          "\"aparc+aseg\""),
                    choices=['wmparc', 'aparc+aseg'],
                    default='wmparc', metavar='STR')
inputs_group.add_argument("--surface_labels",
                    help=("Surface labels: \"freesurfer\" (default) or "
                          "\"atlas\" (for output from older versions of "
                          "FreeSurfer; expects default $SUBJECTS_DIR)"),
                    choices=['freesurfer', 'atlas', 'manual'],
                    default='freesurfer', metavar='STR')
inputs_group.add_argument("--my_segments",
                    help=("Use this segmented file, but still call "
                          "--ants_segments to fetch ANTs transforms"),
                    metavar='STR')
inputs_group.add_argument("--atlases", help=("Extra label volume atlas(es)"
                                             " in MNI152 space"),
                    nargs='+', metavar='')

outputs_group.add_argument("--out",
                    help='Output directory (default: $HOME/mindboggled)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggled'), metavar='STR')
outputs_group.add_argument("--working",
                    help='Working directory '
                         '(default: $HOME/mindboggle_working)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggle_working'), metavar='STR')
outputs_group.add_argument("--cache", help='Download directory '
                                    '(default: $HOME/mindboggle_cache)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggle_cache'), metavar='STR')

extras_group.add_argument("--no_volumes", action='store_true',
                    help="No volume labels, features, or shape tables")
extras_group.add_argument("--no_surfaces", action='store_true',
                    help="No surface labels, features, or shape tables")
extras_group.add_argument("--no_labels", action='store_true',
                    help="No surface or volume labels")
extras_group.add_argument("--no_shapes", action='store_true',
                    help="No shape tables of surface labels or features")
extras_group.add_argument("--save_only_tables", action='store_true',
                    help="Do not save surface or volume files")

extras_group.add_argument("--visual", help=('Generate py/graphviz workflow visual: '
                                      '{hier,flat,exec}'),
                    choices=['hier', 'flat', 'exec'], metavar='STR')
extras_group.add_argument("--cluster", action='store_true',
                    help="Use HTCondor cluster (UNTESTED)")
extras_group.add_argument("--xfm", action='store_true',
                    help="Transform surfaces to MNI152 space")
args = parser.parse_args()

#-----------------------------------------------------------------------------
# Data arguments:
#-----------------------------------------------------------------------------
subject = args.SUBJECT
do_ants = False
subjects_dir = args.subjects_dir
if not subjects_dir:
    subjects_dir = os.environ['SUBJECTS_DIR']
if args.ants_segments:
    ants_segments = args.ants_segments
    do_ants = True
if args.my_segments:
    my_segments = args.my_segments
else:
    my_segments = None
#-----------------------------------------------------------------------------
# Non-FreeSurfer data arguments:
#-----------------------------------------------------------------------------
do_input_vtk = False  # Load VTK surfaces directly (not FreeSurfer surfaces)
do_input_fs_labels = False  # Load nifti (not FreeSurfer mgh file)
use_FS_inputs = True
no_freesurfer_inputs = False #args.no_freesurfer_inputs
if no_freesurfer_inputs:
    use_FS_inputs = False
    do_input_vtk = True
    do_input_fs_labels = True
#-----------------------------------------------------------------------------
# Label and feature arguments:
#-----------------------------------------------------------------------------
overwrite_cerebrum_with_cerebellum = True
fill_noncortex_with_ants_labels = False
volume_labels = args.volume_labels
surface_labels = args.surface_labels
save_all = False
save_only_tables = args.save_only_tables
if not save_only_tables:
    save_all = True
no_surfaces = args.no_surfaces
no_volumes = args.no_volumes
do_sulci = args.sulci
do_fundi = args.fundi
no_labels = args.no_labels
do_smooth_fundi = False
do_label = False
do_surface = False
do_features = False
do_volumes = False
do_shapes = False
if not no_surfaces:
    do_surface = True
if not no_volumes:
    do_volumes = True
if not no_labels:
    do_label = True
if do_sulci or do_fundi:
    do_features = True
#-----------------------------------------------------------------------------
# Shape arguments:
#-----------------------------------------------------------------------------
no_shapes = args.no_shapes
do_spectra = False  # Measure Laplace-Beltrami spectra for labels/features
do_zernike = False  # Compute Zernike moments for labels/features
if not no_shapes or do_features:
    do_shapes = True
if (do_label or do_features) and do_shapes:
    if args.spectra > 0:
        do_spectra = True
    if args.moments > 0:
        do_zernike = True
do_freesurfer_thickness = False  # Include FreeSurfer's thickness measure
do_freesurfer_convexity = False  # Include FreeSurfer's convexity measure
if do_shapes and use_FS_inputs:
    do_freesurfer_thickness = True
    do_freesurfer_convexity = True
#-----------------------------------------------------------------------------
# Basic functions
#-----------------------------------------------------------------------------
def split_list_pair(List):
    element1 = List[0]
    element2 = List[1]
    return element1, element2

def first_string_containing_substring(substring, List):
    first_matching_string = [x for x in List if substring in x][0]
    return first_matching_string

#=============================================================================
#
#   Hidden arguments: paths, label and template data
#
#=============================================================================
#-----------------------------------------------------------------------------
# Path to C++ code:
#-----------------------------------------------------------------------------
ccode_path = os.environ['MINDBOGGLE_TOOLS']  # Mindboggle C++ code directory
#-----------------------------------------------------------------------------
# Hashes to verify retrieved data, and output, working, and cache directories:
#-----------------------------------------------------------------------------
hashes, url, cache_env, cache = hashes_url()
if args.cache:
    cache = args.cache
elif cache_env in os.environ.keys():
    cache = os.environ[cache_env]
if args.working:
    working = os.path.join(args.working, subject)
else:
    working = os.path.join(os.environ['HOME'], 'mindboggle_working', subject)
if not os.path.isdir(args.out):
    print("Create missing output directory: {0}".format(args.out))
    os.makedirs(args.out)
if not os.path.isdir(working):
    print("Create missing working directory: {0}".format(working))
    os.makedirs(working)
if not os.path.isdir(cache):
    print("Create missing cache directory: {0}".format(cache))
    os.makedirs(cache)
#-----------------------------------------------------------------------------
# Labeling protocol information and volume atlases:
#-----------------------------------------------------------------------------
dkt = DKTprotocol()
atlas_volume = 'OASIS-TRT-20_jointfusion_DKT31_CMA_labels_in_MNI152.nii.gz'
atlases = args.atlases
add_atlas_names = []
if atlases:
    if isinstance(atlases, str):
        atlases = [atlases]
    for add_atlas in atlases:
        add_atlas_names.append(os.path.basename(add_atlas).split('.')[0])
atropos_to_MNI152_affine = 'OASIS-30_Atropos_template_to_MNI152_affine.txt'
#-----------------------------------------------------------------------------
# Surface atlas labels:
# - 'manual': manual edits
# - FUTURE: <'adjusted': manual edits after automated alignment to fundi>
#-----------------------------------------------------------------------------
surface_classifier = 'DKTatlas40'
surface_atlas_type = 'manual'
modify_surface_labels = False
#-----------------------------------------------------------------------------
# Evaluation
#-----------------------------------------------------------------------------
do_evaluate_surf_labels = False  # Surface overlap: auto vs. manual labels
do_evaluate_vol_labels = False  # Volume overlap: auto vs. manual labels

#=============================================================================
#
#   Initialize workflow inputs and outputs
#
#=============================================================================
mbFlow = Workflow(name='Mindboggle')
mbFlow.base_dir = working

#-----------------------------------------------------------------------------
# Iterate inputs over hemispheres and atlases
# (surfaces are assumed to take the form: lh.pial or lh.pial.vtk)
#-----------------------------------------------------------------------------
if add_atlas_names:
    InputAddAtlases = Node(name='Input_volume_atlases',
                           interface=IdentityInterface(fields=['atlas']))
    InputAddAtlases.iterables = ('atlas', add_atlas_names)
InputHemis = Node(name='Input_hemispheres',
                  interface=IdentityInterface(fields=['hemi']))
hemis = ['lh', 'rh']
InputHemis.iterables = ('hemi', hemis)
#-----------------------------------------------------------------------------
# Outputs and name substitutions
#-----------------------------------------------------------------------------
Sink = Node(DataSink(), name='Results')
Sink.inputs.base_directory = args.out
Sink.inputs.container = subject

if my_segments:
    seg_in = os.path.basename(my_segments)
else:
    seg_in = 'combined_segmentations.nii.gz'
fs_in = volume_labels + '.nii.gz'
fs_out = 'FreeSurfer_' + volume_labels + '_labels.nii.gz'
fs_filled_in = fs_out +'_to_' + fs_out + '_through_' + seg_in + '_to_' + \
    fs_out + '_through_' + seg_in
fs_filled_out = 'FreeSurfer_' + volume_labels + '_filled_labels.nii.gz'
ants_filled_in = fs_out + '_to_' + \
    'ANTs_labels.nii.gz_to_ANTs_labels.nii.gz_through_' + seg_in
ants_filled_out = 'ANTs_filled_labels.nii.gz'

Sink.inputs.substitutions = [ ('lh.', ''), ('rh.', ''),
    ('_hemi_lh', 'left_surface'), ('_hemi_rh', 'right_surface'),
    ('pial.', ''),
    ('sulc.vtk', 'FreeSurfer_convexity.vtk'),
    ('thickness.vtk', 'FreeSurfer_thickness.vtk'),
    ('relabeled_aparc.vtk', 'FreeSurfer_cortex_labels.vtk'),
    (fs_in, fs_out),
    (fs_filled_in, fs_filled_out),
    (ants_filled_in, ants_filled_out),
    ('smooth_skeletons.vtk', 'smooth_fundi.vtk')]
Sink.inputs.regexp_substitutions = [
    (r'/_atlas_(.*)/ANTs_added_atlas_labels.nii.gz',
     r'/\1_labels.nii.gz'),
    (r'/_atlas_(.*)/volumes.csv',
     r'/\1_labels_volumes.csv')]

#-----------------------------------------------------------------------------
# ANTs transforms for saving MNI152-transformed coordinates and for labeling
#-----------------------------------------------------------------------------
if do_ants:
    #-------------------------------------------------------------------------
    # Retrieve atlas path:
    #-------------------------------------------------------------------------
    FetchAtlas = Node(name='Fetch_atlas',
                      interface=Fn(function=retrieve_data,
                                   input_names=['data_file',
                                                'url',
                                                'hashes',
                                                'cache_env',
                                                'cache',
                                                'return_missing',
                                                'lookup'],
                                   output_names=['data_path']))
    mbFlow.add_nodes([FetchAtlas])
    FetchAtlas.inputs.data_file = atlas_volume
    FetchAtlas.inputs.url = url
    FetchAtlas.inputs.hashes = hashes
    FetchAtlas.inputs.cache_env = cache_env
    FetchAtlas.inputs.cache = cache
    FetchAtlas.inputs.return_missing = False
    FetchAtlas.inputs.lookup = True
    #-------------------------------------------------------------------------
    # Retrieve ANTs data:
    #-------------------------------------------------------------------------
    FetchANTs = Node(name='Fetch_ants_data',
                     interface=Fn(function=fetch_ants_data,
                                  input_names=['segmented_file'],
                                  output_names=['mask',
                                                'segments',
                                                'affine_subject2template',
                                                'warp_subject2template',
                                                'affine_template2subject',
                                                'warp_template2subject']))
    mbFlow.add_nodes([FetchANTs])
    FetchANTs.inputs.segmented_file = ants_segments
    #-------------------------------------------------------------------------
    # Compose single affine transform from subject to MNI152:
    #-------------------------------------------------------------------------
    if (do_shapes and do_surface) or do_label:
        affine_to_mni = retrieve_data(atropos_to_MNI152_affine,
                                      url, hashes, cache_env, cache)
    if do_shapes and do_surface:
        AffineFileList = Node(name='Merge_affine_file_list',
                              interface=Fn(function=list_strings,
                                           input_names=['string1',
                                                        'string2',
                                                        'string3',
                                                        'string4'],
                                           output_names=['string_list']))
        AffineFileList.inputs.string1 = affine_to_mni
        mbFlow.connect(FetchANTs, 'affine_subject2template',
                       AffineFileList, 'string2')
        AffineFileList.inputs.string3 = ''
        AffineFileList.inputs.string4 = ''

        ComposeAffine = Node(name='Compose_affine_transform',
                             interface=Fn(function=ComposeMultiTransform,
                                          input_names=['transform_files',
                                                       'inverse_Booleans',
                                                       'output_transform_file',
                                                       'ext'],
                                          output_names=['output_transform_file']))
        mbFlow.add_nodes([ComposeAffine])
        mbFlow.connect(AffineFileList, 'string_list',
                       ComposeAffine, 'transform_files')
        ComposeAffine.inputs.inverse_Booleans = [False, False]
        ComposeAffine.inputs.output_transform_file = ''
        ComposeAffine.inputs.ext = '.txt'
    #-------------------------------------------------------------------------
    # Construct ANTs MNI152-to-subject nonlinear transform lists:
    #-------------------------------------------------------------------------
    if do_label:
        WarpToSubjectFileList = Node(name='Merge_warp_file_list',
                                     interface=Fn(function=list_strings,
                                          input_names=['string1',
                                                       'string2',
                                                       'string3',
                                                       'string4'],
                                          output_names=['string_list']))
        mbFlow.connect(FetchANTs, 'affine_template2subject',
                       WarpToSubjectFileList, 'string1')
        mbFlow.connect(FetchANTs, 'warp_template2subject',
                       WarpToSubjectFileList, 'string2')
        WarpToSubjectFileList.inputs.string3 = affine_to_mni
        WarpToSubjectFileList.inputs.string4 = ''
        warp_inverse_Booleans = [False, False, True]

#=============================================================================
#-----------------------------------------------------------------------------
#
#   Surface workflows
#
#-----------------------------------------------------------------------------
#=============================================================================
if do_surface:
    #-------------------------------------------------------------------------
    # Location and structure of the surface inputs:
    #-------------------------------------------------------------------------
    use_white_surface = False
    if use_white_surface:
        Surf = Node(name='Surfaces',
                    interface=DataGrabber(infields=['subject', 'hemi'],
                                          outfields=['surface_files',
                                                     'white_surface_files'],
                                          sort_filelist=False))
    else:
        Surf = Node(name='Surfaces',
                    interface=DataGrabber(infields=['subject', 'hemi'],
                                          outfields=['surface_files'],
                                          sort_filelist=False))
    Surf.inputs.base_directory = subjects_dir
    Surf.inputs.template = '%s/surf/%s.%s'
    Surf.inputs.template_args['surface_files'] = [['subject', 'hemi', 'pial']]
    if use_white_surface:
        Surf.inputs.template_args['white_surface_files'] = [['subject',
                                                             'hemi', 'white']]
    #Surf.inputs.template_args['sphere_files'] = [['subject','hemi','sphere']]
    if do_freesurfer_thickness:
        Surf.inputs.template_args['freesurfer_thickness_files'] = \
            [['subject', 'hemi', 'thickness']]
    if do_freesurfer_convexity:
        Surf.inputs.template_args['freesurfer_convexity_files'] = \
            [['subject', 'hemi', 'sulc']]

    Surf.inputs.subject = subject
    mbFlow.connect(InputHemis, 'hemi', Surf, 'hemi')
    #-------------------------------------------------------------------------
    # Convert surfaces to VTK:
    #-------------------------------------------------------------------------
    if not do_input_vtk:
        ConvertSurf = Node(name='Surface_to_vtk',
                           interface=Fn(function=surface_to_vtk,
                                        input_names=['surface_file',
                                                     'output_vtk'],
                                        output_names=['output_vtk']))
        mbFlow.connect(Surf, 'surface_files', ConvertSurf, 'surface_file')
        ConvertSurf.inputs.output_vtk = ''
        if use_white_surface:
            ConvertWhiteSurf = ConvertSurf.clone('Gray-white_surface_to_vtk')
            mbFlow.add_nodes([ConvertWhiteSurf])
            mbFlow.connect(Surf, 'white_surface_files',
                           ConvertWhiteSurf, 'surface_file')
    #-------------------------------------------------------------------------
    # Evaluation inputs: location and structure of atlas surfaces:
    #-------------------------------------------------------------------------
    if (do_evaluate_surf_labels or surface_labels == 'manual') and do_label:
        SurfaceAtlas = Node(name='Surface_atlas',
                            interface=DataGrabber(infields=['subject','hemi'],
                                                  outfields=['atlas_file'],
                                                  sort_filelist=False))
        SurfaceAtlas.inputs.base_directory = subjects_dir
        SurfaceAtlas.inputs.template = '%s/label/%s.labels.DKT31.' +\
                                       surface_atlas_type + '.vtk'
        SurfaceAtlas.inputs.template_args['atlas_file'] = [['subject','hemi']]

        SurfaceAtlas.inputs.subject = subject
        mbFlow.connect(InputHemis, 'hemi', SurfaceAtlas, 'hemi')

    #=========================================================================
    #
    #   Surface labels
    #
    #=========================================================================
    if do_label:
        SurfLabelFlow = Workflow(name='Surface_labels')

        #=====================================================================
        # Initialize labels with the DKT classifier atlas
        #=====================================================================
        if surface_labels == 'atlas' and use_FS_inputs:
            #-----------------------------------------------------------------
            # Label brain with DKT atlas using FreeSurfer's mris_ca_label:
            #-----------------------------------------------------------------
            Classifier = Node(name='mris_ca_label',
                              interface=Fn(function=label_with_classifier,
                                           input_names=['subject',
                                                        'hemi',
                                                        'left_classifier',
                                                        'right_classifier',
                                                        'annot_file',
                                                        'subjects_directory'],
                                           output_names=['annot_file']))
            SurfLabelFlow.add_nodes([Classifier])
            Classifier.inputs.subject = subject
            mbFlow.connect(InputHemis, 'hemi',
                           SurfLabelFlow, 'mris_ca_label.hemi')
            left_classifier_file = 'lh.' + surface_classifier + '.gcs'
            right_classifier_file = 'rh.' + surface_classifier + '.gcs'
            left_classifier = retrieve_data(left_classifier_file, url,
                                            hashes, cache_env, cache)
            right_classifier = retrieve_data(right_classifier_file, url,
                                             hashes, cache_env, cache)
            Classifier.inputs.left_classifier = left_classifier
            Classifier.inputs.right_classifier = right_classifier
            Classifier.inputs.annot_file = ''
            Classifier.inputs.subjects_directory = subjects_dir
            #-----------------------------------------------------------------
            # Convert .annot file to VTK format:
            #-----------------------------------------------------------------
            Classifier2vtk = Node(name='annot_to_vtk',
                                  interface=Fn(function=annot_to_vtk,
                                               input_names=['annot_file',
                                                            'vtk_file'],
                                               output_names=['labels',
                                                             'output_vtk']))
            SurfLabelFlow.add_nodes([Classifier2vtk])
            SurfLabelFlow.connect(Classifier, 'annot_file',
                                  Classifier2vtk, 'annot_file')
            if do_input_vtk:
                mbFlow.connect(Surf, 'surface_files',
                               SurfLabelFlow, 'annot_to_vtk.vtk_file')
            else:
                mbFlow.connect(ConvertSurf, 'output_vtk',
                               SurfLabelFlow, 'annot_to_vtk.vtk_file')
            #if save_all:
            #    mbFlow.connect(SurfLabelFlow, 'annot_to_vtk.output_vtk',
            #                   Sink, 'labels.@DKT_surface')
            plug = 'annot_to_vtk.output_vtk'
            plug1 = Classifier2vtk
            plug2 = 'output_vtk'

        #=====================================================================
        # Initialize labels with FreeSurfer
        #=====================================================================
        elif surface_labels == 'freesurfer' and use_FS_inputs:
            #-----------------------------------------------------------------
            # Location and structure of the FreeSurfer label inputs:
            #-----------------------------------------------------------------
            if use_FS_inputs and do_label and surface_labels == 'freesurfer':
                Annot = Node(name='annot',
                             interface=DataGrabber(infields=['subject',
                                                             'hemi'],
                                                   outfields=['annot_files'],
                                                   sort_filelist=False))
                Annot.inputs.base_directory = subjects_dir
                Annot.inputs.template = '%s/label/%s.aparc.annot'
                Annot.inputs.template_args['annot_files'] = [['subject',
                                                              'hemi']]
                Annot.inputs.subject = subject
                mbFlow.connect(InputHemis, 'hemi', Annot, 'hemi')
            #-----------------------------------------------------------------
            # Convert Annot to VTK format:
            #-----------------------------------------------------------------
            FreeLabels = Node(name='FreeSurfer_annot_to_vtk',
                              interface=Fn(function=annot_to_vtk,
                                           input_names=['annot_file',
                                                        'vtk_file'],
                                           output_names=['labels',
                                                         'output_vtk']))
            SurfLabelFlow.add_nodes([FreeLabels])
            mbFlow.connect(Annot, 'annot_files', SurfLabelFlow,
                           'FreeSurfer_annot_to_vtk.annot_file')
            if do_input_vtk:
                mbFlow.connect(Surf, 'surface_files', SurfLabelFlow,
                               'FreeSurfer_annot_to_vtk.vtk_file')
            else:
                mbFlow.connect(ConvertSurf, 'output_vtk', SurfLabelFlow,
                               'FreeSurfer_annot_to_vtk.vtk_file')
            plug = 'FreeSurfer_annot_to_vtk.output_vtk'
            plug1 = FreeLabels
            plug2 = 'output_vtk'

        #=====================================================================
        # Skip label initialization and process manual (atlas) labels
        #=====================================================================
        elif surface_labels == 'manual':
            ManualSurfLabels = Node(name='Manual_surface_labels',
                                    interface=Fn(function=read_vtk,
                                                 input_names=['input_vtk',
                                                              'return_first',
                                                              'return_array'],
                                                 output_names=['faces',
                                                               'lines',
                                                               'indices',
                                                               'points',
                                                               'npoints',
                                                               'scalars',
                                                               'scalar_names',
                                                               'input_vtk']))
            SurfLabelFlow.add_nodes([ManualSurfLabels])
            mbFlow.connect(SurfaceAtlas, 'atlas_file',
                           SurfLabelFlow, 'Manual_surface_labels.input_vtk')
            ManualSurfLabels.inputs.return_first = 'True'
            ManualSurfLabels.inputs.return_array = 'False'
            plug = 'Manual_surface_labels.input_vtk'
            plug1 = ManualSurfLabels
            plug2 = 'input_vtk'

        ##=====================================================================
        ## Surface label evaluation against manual labels
        ##=====================================================================
        #if do_evaluate_surf_labels:
        #
        #    EvalSurfLabels = Node(name='Evaluate_surface_labels',
        #                          interface=Fn(function=measure_surface_overlap,
        #                                       input_names=['command',
        #                                                    'labels_file1',
        #                                                    'labels_file2'],
        #                                       output_names=['overlap_file']))
        #    mbFlow.add_nodes([EvalSurfLabels])
        #    surface_overlap_command = os.path.join(ccode_path,
        #        'surface_overlap', 'SurfaceOverlapMain')
        #    EvalSurfLabels.inputs.command = surface_overlap_command
        #    mbFlow.connect(SurfaceAtlas, 'atlas_file',
        #                   EvalSurfLabels, 'labels_file1')
        #    mbFlow.connect(SurfLabelFlow, plug,
        #                   'EvalSurfLabels.labels_file2')

        #=====================================================================
        # Convert surface label numbers to volume label numbers
        #=====================================================================
        ReindexLabels = Node(name='Reindex_labels',
                             interface=Fn(function=relabel_surface,
                                          input_names=['vtk_file',
                                                       'hemi',
                                                       'old_labels',
                                                       'new_labels',
                                                       'erase_labels',
                                                       'erase_value',
                                                       'output_file'],
                                          output_names=['output_file']))
        SurfLabelFlow.add_nodes([ReindexLabels])
        SurfLabelFlow.connect(plug1, plug2, ReindexLabels, 'vtk_file')
        mbFlow.connect(InputHemis, 'hemi',
                       SurfLabelFlow, 'Reindex_labels.hemi')
        ReindexLabels.inputs.old_labels = ''
        ReindexLabels.inputs.new_labels = ''
        ReindexLabels.inputs.erase_labels = [-1, 0]
        ReindexLabels.inputs.erase_value = -1
        ReindexLabels.inputs.output_file = ''
        if save_all:
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           Sink, 'labels.@surface')

    #=========================================================================
    #
    #   Surface shape measurements
    #
    #=========================================================================
    if do_shapes:
        WholeSurfShapeFlow = Workflow(name='Surface_shapes')
        #---------------------------------------------------------------------
        # Measure surface area:
        #---------------------------------------------------------------------
        SurfaceArea = Node(name='Surface_area',
                           interface=Fn(function=area,
                                        input_names=['command',
                                                     'surface_file'],
                                        output_names=['area_file']))
        area_command = os.path.join(ccode_path, 'area', 'PointAreaMain')
        SurfaceArea.inputs.command = area_command
        #---------------------------------------------------------------------
        # Measure surface travel depth:
        #---------------------------------------------------------------------
        TravelDepth = Node(name='Travel_depth',
                           interface=Fn(function=travel_depth,
                                        input_names=['command',
                                                     'surface_file'],
                                        output_names=['depth_file']))
        WholeSurfShapeFlow.add_nodes([TravelDepth])
        TravelDepth.inputs.command = os.path.join(ccode_path,
                                                  'travel_depth',
                                                  'TravelDepthMain')
        #---------------------------------------------------------------------
        # Rescale surface travel depth:
        #---------------------------------------------------------------------
        if do_fundi:
            RescaleTravelDepth = Node(name='Rescale_travel_depth',
                                interface=Fn(function=rescale_by_neighborhood,
                                     input_names=['input_vtk',
                                                  'indices',
                                                  'nedges',
                                                  'p',
                                                  'set_max_to_1',
                                                  'save_file',
                                                  'output_filestring'],
                                     output_names=['rescaled_scalars',
                                                   'rescaled_scalars_file']))
            WholeSurfShapeFlow.add_nodes([RescaleTravelDepth])
            WholeSurfShapeFlow.connect(TravelDepth, 'depth_file',
                                       RescaleTravelDepth, 'input_vtk')
            RescaleTravelDepth.inputs.indices = []
            RescaleTravelDepth.inputs.nedges = 10
            RescaleTravelDepth.inputs.p = 99
            RescaleTravelDepth.inputs.set_max_to_1 = True
            RescaleTravelDepth.inputs.save_file = True
            RescaleTravelDepth.inputs.output_filestring = \
                'travel_depth_rescaled'
        #---------------------------------------------------------------------
        # Measure surface geodesic depth:
        #---------------------------------------------------------------------
        GeodesicDepth = Node(name='Geodesic_depth',
                             interface=Fn(function=geodesic_depth,
                                          input_names=['command',
                                                       'surface_file'],
                                          output_names=['depth_file']))
        GeodesicDepth.inputs.command = os.path.join(ccode_path,
                                                    'geodesic_depth',
                                                    'GeodesicDepthMain')
        #---------------------------------------------------------------------
        # Measure surface curvature:
        #---------------------------------------------------------------------
        CurvNode = Node(name='Curvature',
                        interface=Fn(function=curvature,
                             input_names=['command',
                                          'method',
                                          'arguments',
                                          'surface_file'],
                             output_names=['mean_curvature_file',
                                           'gauss_curvature_file',
                                           'max_curvature_file',
                                           'min_curvature_file',
                                           'min_curvature_vector_file']))
        CurvNode.inputs.command = os.path.join(ccode_path,
                                               'curvature',
                                               'CurvatureMain')
        CurvNode.inputs.method = 2
        CurvNode.inputs.arguments = '-n 0.7'
        #---------------------------------------------------------------------
        # Convert FreeSurfer surface measures to VTK:
        #---------------------------------------------------------------------
        if do_freesurfer_convexity:
            ConvexNode = Node(name='Convexity_to_vtk',
                              interface=Fn(function=curvature_to_vtk,
                                           input_names=['surface_file',
                                                        'vtk_file',
                                                        'output_vtk'],
                                           output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ConvexNode])
            mbFlow.connect(Surf, 'freesurfer_convexity_files',
                           WholeSurfShapeFlow,
                           'Convexity_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Convexity_to_vtk.vtk_file')
            ConvexNode.inputs.output_vtk = ''
            if save_all:
                mbFlow.connect(WholeSurfShapeFlow, 'Convexity_to_vtk.output_vtk',
                               Sink, 'shapes.@freesurfer_convexity')
        if do_freesurfer_thickness:
            ThickNode = Node(name='Thickness_to_vtk',
                             interface=Fn(function=curvature_to_vtk,
                                          input_names=['surface_file',
                                                       'vtk_file',
                                                       'output_vtk'],
                                          output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ThickNode])
            mbFlow.connect(Surf, 'freesurfer_thickness_files',
                           WholeSurfShapeFlow,
                           'Thickness_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Thickness_to_vtk.vtk_file')
            ThickNode.inputs.output_vtk = ''
            if save_all:
                mbFlow.connect(WholeSurfShapeFlow, 'Thickness_to_vtk.output_vtk',
                               Sink, 'shapes.@freesurfer_thickness')
        #---------------------------------------------------------------------
        # Connect nodes:
        #---------------------------------------------------------------------
        WholeSurfShapeFlow.add_nodes([SurfaceArea, GeodesicDepth, CurvNode])
        if do_input_vtk:
            mbFlow.connect([(Surf, WholeSurfShapeFlow,
                             [('surface_files','Surface_area.surface_file'),
                              ('surface_files','Travel_depth.surface_file'),
                              ('surface_files','Geodesic_depth.surface_file'),
                              ('surface_files','Curvature.surface_file')])])
        else:
            mbFlow.connect([(ConvertSurf, WholeSurfShapeFlow,
                               [('output_vtk', 'Surface_area.surface_file'),
                                ('output_vtk', 'Travel_depth.surface_file'),
                                ('output_vtk', 'Geodesic_depth.surface_file'),
                                ('output_vtk', 'Curvature.surface_file')])])
        if save_all:
            mbFlow.connect([(WholeSurfShapeFlow, Sink,
               [('Surface_area.area_file', 'shapes.@surface_area'),
                ('Travel_depth.depth_file', 'shapes.@travel_depth'),
                ('Geodesic_depth.depth_file', 'shapes.@geodesic_depth'),
                ('Curvature.mean_curvature_file', 'shapes.@mean_curvature')])])

    #=========================================================================
    #
    #   Surface feature extraction
    #
    #=========================================================================
    if do_features:
        SurfFeatureFlow = Workflow(name='Surface_features')

        #=====================================================================
        # Folds and sulci
        #=====================================================================
        if do_sulci:
            #-----------------------------------------------------------------
            # Folds:
            #-----------------------------------------------------------------
            FoldsNode = Node(name='Folds',
                             interface=Fn(function=extract_folds,
                                          input_names=['depth_file',
                                                       'min_fold_size',
                                                       'tiny_depth',
                                                       'save_file'],
                                          output_names=['folds',
                                                        'n_folds',
                                                        'depth_threshold',
                                                        'bins',
                                                        'bin_edges',
                                                        'folds_file']))
            SurfFeatureFlow.add_nodes([FoldsNode])
            mbFlow.connect(WholeSurfShapeFlow, 'Travel_depth.depth_file',
                           SurfFeatureFlow, 'Folds.depth_file')
            FoldsNode.inputs.min_fold_size = 50
            FoldsNode.inputs.tiny_depth = 0.001
            FoldsNode.inputs.save_file = True
            #if save_all:
            #   mbFlow.connect(SurfFeatureFlow, 'Folds.folds_file',
            #                  Sink, 'features.@folds')
            #-----------------------------------------------------------------
            # Sulci:
            #-----------------------------------------------------------------
            SulciNode = Node(name='Sulci',
                             interface=Fn(function=extract_sulci,
                                          input_names=['labels_file',
                                                       'folds_or_file',
                                                       'hemi',
                                                       'min_boundary',
                                                       'sulcus_names'],
                                          output_names=['sulci',
                                                        'n_sulci',
                                                        'sulci_file']))
            SurfFeatureFlow.add_nodes([SulciNode])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureFlow, 'Sulci.labels_file')
            SurfFeatureFlow.connect(FoldsNode, 'folds',
                                    SulciNode, 'folds_or_file')
            mbFlow.connect(InputHemis, 'hemi', SurfFeatureFlow, 'Sulci.hemi')
            SulciNode.inputs.min_boundary = 1
            SulciNode.inputs.sulcus_names = dkt.sulcus_names
            if save_all:
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               Sink, 'features.@sulci')

        #=====================================================================
        # Fundi
        #=====================================================================
        if do_fundi:
            #-----------------------------------------------------------------
            # Extract a fundus per fold:
            #-----------------------------------------------------------------
            FoldFundi = Node(name='Fundus_per_fold',
                             interface=Fn(function=extract_fundi,
                                  input_names=['folds',
                                               'curv_file',
                                               'depth_file',
                                               'min_separation',
                                               'erode_ratio',
                                               'erode_min_size',
                                               'save_file'],
                                  output_names=['fundus_per_fold',
                                                'n_fundi_in_folds',
                                                'fundus_per_fold_file']))
            SurfFeatureFlow.connect(FoldsNode, 'folds', FoldFundi, 'folds')
            mbFlow.connect([(WholeSurfShapeFlow, SurfFeatureFlow,
                           [('Curvature.mean_curvature_file',
                             'Fundus_per_fold.curv_file'),
                            ('Rescale_travel_depth.rescaled_scalars_file',
                             'Fundus_per_fold.depth_file')])])
            FoldFundi.inputs.min_separation = 10
            FoldFundi.inputs.erode_ratio = 0.10
            FoldFundi.inputs.erode_min_size = 10
            FoldFundi.inputs.save_file = False
            #if save_all:
            #    mbFlow.connect(SurfFeatureFlow,
            #                   'Fundus_per_fold.fundus_per_fold_file',
            #                   Sink, 'features.@fundus_per_fold')

            if do_smooth_fundi:
                #-------------------------------------------------------------
                # Compute likelihoods for smoothing fundi:
                #-------------------------------------------------------------
                LikelihoodNode = Node(name='Likelihood',
                    interface=Fn(function=compute_likelihood,
                                 input_names=['trained_file',
                                              'depth_file',
                                              'curvature_file',
                                              'folds',
                                              'save_file'],
                                 output_names=['likelihoods',
                                               'likelihoods_file']))
                SurfFeatureFlow.add_nodes([LikelihoodNode])
                border_params_file = \
                    'depth_curv_border_nonborder_parameters.pkl'
                border_params_path = retrieve_data(border_params_file, url,
                                                   hashes, cache_env, cache)
                LikelihoodNode.inputs.trained_file = border_params_path
                mbFlow.connect([(WholeSurfShapeFlow, SurfFeatureFlow,
                    [('Rescale_travel_depth.rescaled_scalars_file',
                      'Likelihood.depth_file'),
                     ('Curvature.mean_curvature_file',
                      'Likelihood.curvature_file')])])
                SurfFeatureFlow.connect(FoldsNode, 'folds',
                                        LikelihoodNode, 'folds')
                LikelihoodNode.inputs.save_file = True
                #if save_all:
                #    mbFlow.connect(SurfFeatureFlow, 'Likelihood.likelihoods_file',
                #                   Sink, 'features.@likelihoods')
                #-------------------------------------------------------------
                # Smooth fundi:
                #-------------------------------------------------------------
                SmoothFundi = Node(name='Smooth_fundi',
                                   interface=Fn(function=smooth_skeleton,
                                        input_names=['skeletons',
                                                     'bounds',
                                                     'vtk_file',
                                                     'likelihoods',
                                                     'wN_max',
                                                     'erode_again',
                                                     'save_file'],
                                        output_names=['smooth_skeletons',
                                                      'n_skeletons',
                                                      'skeletons_file']))
                SurfFeatureFlow.connect(FoldFundi, 'fundus_per_fold',
                                        SmoothFundi, 'skeletons')
                SurfFeatureFlow.connect(FoldsNode, 'folds',
                                        SmoothFundi, 'bounds')
                mbFlow.connect(WholeSurfShapeFlow,
                               'Curvature.mean_curvature_file',
                               SurfFeatureFlow, 'Smooth_fundi.vtk_file')
                SurfFeatureFlow.connect(LikelihoodNode, 'likelihoods',
                                        SmoothFundi, 'likelihoods')
                SmoothFundi.inputs.wN_max = 1.0
                SmoothFundi.inputs.erode_again = False
                SmoothFundi.inputs.save_file = True
                if save_all:
                    mbFlow.connect(SurfFeatureFlow, 'Smooth_fundi.skeletons_file',
                                   Sink, 'features.@smooth_fundi')

            #-----------------------------------------------------------------
            # Segment a fundus per sulcus:
            #-----------------------------------------------------------------
            SulcusFundi = Node(name='Fundus_per_sulcus',
                               interface=Fn(function=segment_fundi,
                                    input_names=['fundus_per_fold',
                                                 'sulci',
                                                 'vtk_file',
                                                 'save_file'],
                                    output_names=['fundus_per_sulcus',
                                                  'n_fundi',
                                                  'fundus_per_sulcus_file']))
            if do_smooth_fundi:
                SurfFeatureFlow.connect(SmoothFundi, 'smooth_skeletons',
                                        SulcusFundi, 'fundus_per_fold')
            else:
                SurfFeatureFlow.connect(FoldFundi, 'fundus_per_fold',
                                        SulcusFundi, 'fundus_per_fold')
            SurfFeatureFlow.connect(SulciNode, 'sulci', SulcusFundi, 'sulci')
            mbFlow.connect(WholeSurfShapeFlow,
                           'Curvature.mean_curvature_file',
                           SurfFeatureFlow, 'Fundus_per_sulcus.vtk_file')
            SulcusFundi.inputs.save_file = True
            if save_all:
                mbFlow.connect(SurfFeatureFlow,
                               'Fundus_per_sulcus.fundus_per_sulcus_file',
                               Sink, 'features.@fundus_per_sulcus')

    #=========================================================================
    #
    #   Surface feature shapes
    #
    #=========================================================================
    if do_shapes:
        SurfFeatureShapeFlow = Workflow(name='Surface_feature_shapes')
        #=====================================================================
        # Compute Laplace-Beltrami spectra
        #=====================================================================
        if do_spectra:
            #-----------------------------------------------------------------
            # Measure spectra of labeled regions:
            #-----------------------------------------------------------------
            SpectraLabels = Node(name='Spectra_labels',
                                 interface=Fn(function=spectrum_per_label,
                                              input_names=['vtk_file',
                                                           'spectrum_size',
                                                           'exclude_labels',
                                                           'normalization',
                                                           'area_file',
                                                           'largest_segment'],
                                              output_names=['spectrum_lists',
                                                            'label_list']))
            SurfFeatureShapeFlow.add_nodes([SpectraLabels])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureShapeFlow, 'Spectra_labels.vtk_file')
            SpectraLabels.inputs.spectrum_size = args.spectra
            SpectraLabels.inputs.exclude_labels = [0]
            SpectraLabels.inputs.normalization = "area"
            SpectraLabels.inputs.area_file = ""
            SpectraLabels.inputs.largest_segment = True
            mbFlow.connect(WholeSurfShapeFlow, 'Surface_area.area_file',
                           SurfFeatureShapeFlow, 'Spectra_labels.area_file')
            #-----------------------------------------------------------------
            # Compute spectra of sulci:
            #-----------------------------------------------------------------
            if do_sulci:
                SpectraSulci = SpectraLabels.clone('Spectra_sulci')
                SurfFeatureShapeFlow.add_nodes([SpectraSulci])
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               SurfFeatureShapeFlow, 'Spectra_sulci.vtk_file')
                SpectraSulci.inputs.exclude_labels = [-1]

        #=====================================================================
        # Compute Zernike moments
        #=====================================================================
        if do_zernike:
            #-----------------------------------------------------------------
            # Measure Zernike moments of labeled regions:
            #-----------------------------------------------------------------
            ZernikeLabels = Node(name='Zernike_labels',
                 interface=Fn(function=zernike_moments_per_label,
                              input_names=['vtk_file',
                                           'order',
                                           'exclude_labels',
                                           'scale_input',
                                           'decimate_fraction',
                                           'decimate_smooth'],
                              output_names=['descriptors_lists',
                                            'label_list']))
            SurfFeatureShapeFlow.add_nodes([ZernikeLabels])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureShapeFlow, 'Zernike_labels.vtk_file')
            ZernikeLabels.inputs.order = args.moments
            ZernikeLabels.inputs.exclude_labels = [0]
            ZernikeLabels.inputs.scale_input = True
            ZernikeLabels.inputs.decimate_fraction = 0
            ZernikeLabels.inputs.decimate_smooth = 0
            #-----------------------------------------------------------------
            # Compute Zernike moments of sulci:
            #-----------------------------------------------------------------
            if do_sulci:
                ZernikeSulci = ZernikeLabels.clone('Zernike_sulci')
                SurfFeatureShapeFlow.add_nodes([ZernikeSulci])
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               SurfFeatureShapeFlow, 'Zernike_sulci.vtk_file')
                ZernikeSulci.inputs.exclude_labels = [-1]

    #=========================================================================
    #
    #   Surface feature shape tables
    #
    #=========================================================================
    if do_shapes:
        #---------------------------------------------------------------------
        # Surface feature shape tables: labels, sulci, fundi:
        #---------------------------------------------------------------------
        ShapeTables = Node(name='Shape_tables',
                           interface=Fn(function=write_shape_stats,
                                input_names=['labels_or_file',
                                             'sulci',
                                             'fundi',
                                             'affine_transform_file',
                                             'transform_format',
                                             'area_file',
                                             'normalize_by_area',
                                             'mean_curvature_file',
                                             'travel_depth_file',
                                             'geodesic_depth_file',
                                             'freesurfer_convexity_file',
                                             'freesurfer_thickness_file',
                                             'labels_spectra',
                                             'labels_spectra_IDs',
                                             'sulci_spectra',
                                             'sulci_spectra_IDs',
                                             'labels_zernike',
                                             'labels_zernike_IDs',
                                             'sulci_zernike',
                                             'sulci_zernike_IDs',
                                             'exclude_labels',
                                             'delimiter'],
                                output_names=['label_table',
                                              'sulcus_table',
                                              'fundus_table']))
        mbFlow.add_nodes([ShapeTables])
        ShapeTables.inputs.labels_or_file = []
        ShapeTables.inputs.sulci = []
        ShapeTables.inputs.fundi = []
        ShapeTables.inputs.affine_transform_file = None
        ShapeTables.inputs.transform_format = None
        ShapeTables.inputs.freesurfer_convexity_file = ''
        ShapeTables.inputs.freesurfer_thickness_file = ''
        ShapeTables.inputs.labels_spectra = []
        ShapeTables.inputs.sulci_spectra = []
        ShapeTables.inputs.labels_spectra_IDs = []
        ShapeTables.inputs.sulci_spectra_IDs = []
        ShapeTables.inputs.labels_zernike = []
        ShapeTables.inputs.sulci_zernike = []
        ShapeTables.inputs.labels_zernike_IDs = []
        ShapeTables.inputs.sulci_zernike_IDs = []
        if do_label:
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           ShapeTables, 'labels_or_file')
        if do_sulci:
            mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci',
                           ShapeTables, 'sulci')
        if do_fundi:
            mbFlow.connect(SurfFeatureFlow,
                           'Fundus_per_sulcus.fundus_per_sulcus',
                           ShapeTables, 'fundi')
        if do_ants:
            mbFlow.connect(ComposeAffine, 'output_transform_file',
                           ShapeTables, 'affine_transform_file')
            ShapeTables.inputs.transform_format = 'itk'
        ShapeTables.inputs.normalize_by_area = True
        mbFlow.connect([(WholeSurfShapeFlow, ShapeTables,
                   [('Surface_area.area_file', 'area_file'),
                    ('Curvature.mean_curvature_file', 'mean_curvature_file'),
                    ('Travel_depth.depth_file', 'travel_depth_file'),
                    ('Geodesic_depth.depth_file', 'geodesic_depth_file')])])
        if do_freesurfer_convexity:
            mbFlow.connect(WholeSurfShapeFlow, 'Convexity_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_convexity_file')
        if do_freesurfer_thickness:
            mbFlow.connect(WholeSurfShapeFlow, 'Thickness_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_thickness_file')

        # Laplace-Beltrami spectra:
        if do_spectra:
            mbFlow.connect(SurfFeatureShapeFlow,
                           'Spectra_labels.spectrum_lists',
                           ShapeTables, 'labels_spectra')
            mbFlow.connect(SurfFeatureShapeFlow, 'Spectra_labels.label_list',
                           ShapeTables, 'labels_spectra_IDs')
            if do_sulci:
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Spectra_sulci.spectrum_lists',
                               ShapeTables, 'sulci_spectra')
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Spectra_sulci.label_list',
                               ShapeTables, 'sulci_spectra_IDs')
            else:
                ShapeTables.inputs.sulci_spectra = []
                ShapeTables.inputs.sulci_spectra_IDs = []

        # Zernike moments:
        if do_zernike:
            mbFlow.connect(SurfFeatureShapeFlow,
                           'Zernike_labels.descriptors_lists',
                           ShapeTables, 'labels_zernike')
            mbFlow.connect(SurfFeatureShapeFlow, 'Zernike_labels.label_list',
                           ShapeTables, 'labels_zernike_IDs')
            if do_sulci:
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Zernike_sulci.descriptors_lists',
                               ShapeTables, 'sulci_zernike')
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Zernike_sulci.label_list',
                               ShapeTables, 'sulci_zernike_IDs')
            else:
                ShapeTables.inputs.sulci_zernike = []
                ShapeTables.inputs.sulci_zernike_IDs = []

        ShapeTables.inputs.exclude_labels = [-1]
        ShapeTables.inputs.delimiter = ","
        mbFlow.connect(ShapeTables, 'label_table', Sink, 'tables.@labels')
        if do_sulci:
            mbFlow.connect(ShapeTables, 'sulcus_table', Sink, 'tables.@sulci')
        if do_fundi:
            mbFlow.connect(ShapeTables, 'fundus_table', Sink, 'tables.@fundi')
        #---------------------------------------------------------------------
        # Vertex measures table:
        #---------------------------------------------------------------------
        if args.vertices:
            VertexTable = Node(name='Vertex_table',
                               interface=Fn(function=write_vertex_measures,
                                    input_names=['output_table',
                                                 'labels_or_file',
                                                 'sulci',
                                                 'fundi',
                                                 'affine_transform_file',
                                                 'transform_format',
                                                 'area_file',
                                                 'mean_curvature_file',
                                                 'travel_depth_file',
                                                 'geodesic_depth_file',
                                                 'freesurfer_convexity_file',
                                                 'freesurfer_thickness_file',
                                                 'delimiter'],
                                    output_names=['output_table']))
            mbFlow.add_nodes([VertexTable])
            VertexTable.inputs.output_table = ''
            VertexTable.inputs.labels_or_file = []
            VertexTable.inputs.sulci = []
            VertexTable.inputs.fundi = []
            VertexTable.inputs.affine_transform_file = None
            VertexTable.inputs.transform_format = None
            VertexTable.inputs.freesurfer_thickness_file = ''
            VertexTable.inputs.freesurfer_convexity_file = ''
            if do_label:
                mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                               VertexTable, 'labels_or_file')
            if do_sulci:
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci',
                               VertexTable, 'sulci')
            if do_fundi:
                mbFlow.connect(SurfFeatureFlow,
                               'Fundus_per_sulcus.fundus_per_sulcus',
                               VertexTable, 'fundi')

            if do_ants:
                mbFlow.connect(ComposeAffine, 'output_transform_file',
                               VertexTable, 'affine_transform_file')
                VertexTable.inputs.transform_format = 'itk'
            mbFlow.connect([(WholeSurfShapeFlow, VertexTable,
                               [('Surface_area.area_file','area_file'),
                                ('Travel_depth.depth_file',
                                 'travel_depth_file'),
                                ('Geodesic_depth.depth_file',
                                 'geodesic_depth_file'),
                                ('Curvature.mean_curvature_file',
                                 'mean_curvature_file')])])
            if do_freesurfer_thickness:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Thickness_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_thickness_file')
            if do_freesurfer_convexity:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Convexity_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_convexity_file')

            VertexTable.inputs.delimiter = ","
            mbFlow.connect(VertexTable, 'output_table',
                           Sink, 'tables.@vertices')

        #---------------------------------------------------------------------
        # Apply affine transform to surface coordinates:
        #---------------------------------------------------------------------
        if do_ants and args.xfm:
            TransformVTK = Node(name='Transform_surface_points',
                                interface=Fn(function=apply_affine_transform,
                                             input_names=['transform_file',
                                                          'vtk_or_points',
                                                          'transform_format',
                                                          'vtk_file_stem'],
                                             output_names=['affine_points',
                                                           'output_file']))
            mbFlow.add_nodes([TransformVTK])
            mbFlow.connect(ComposeAffine, 'output_transform_file',
                           TransformVTK, 'transform_file')
            mbFlow.connect(TravelDepth, 'depth_file',
                           TransformVTK, 'vtk_or_points')
            TransformVTK.inputs.transform_format = 'itk'
            TransformVTK.inputs.vtk_file_stem = 'affine_'


#=============================================================================
#-----------------------------------------------------------------------------
#
#   Volume workflows
#
#-----------------------------------------------------------------------------
#=============================================================================
if do_volumes and do_label:

    #=========================================================================
    #
    #   Location and structure of FreeSurfer volume inputs
    #
    #=========================================================================
    #-------------------------------------------------------------------------
    # Original image (.mgz) for converting from conformal (below):
    #-------------------------------------------------------------------------
    mghOrig = Node(name='mgh_orig',
                   interface=DataGrabber(infields=['subject'],
                                         outfields=['mgh_orig'],
                                         sort_filelist=False))
    mghOrig.inputs.base_directory = subjects_dir
    mghOrig.inputs.template = '%s/mri/orig/001.mgz'
    mghOrig.inputs.template_args['mgh_orig'] = [['subject']]
    mghOrig.inputs.subject = subject
    #---------------------------------------------------------------------
    # Convert FreeSurfer mgh conformal file to nifti format:
    #---------------------------------------------------------------------
    MGH2Nifti = Node(name='mgh_to_nifti',
                     interface=Fn(function=convert_mgh_to_native_nifti,
                                  input_names=['input_file',
                                               'reference_file',
                                               'output_file',
                                               'interp'],
                                  output_names=['output_file']))
    mbFlow.connect(mghOrig, 'mgh_orig', MGH2Nifti, 'input_file')
    mbFlow.connect(mghOrig, 'mgh_orig', MGH2Nifti, 'reference_file')
    MGH2Nifti.inputs.output_file = ''
    MGH2Nifti.inputs.interp = 'trilin'
    #-------------------------------------------------------------------------
    # Use own whole-brain nifti label volume:
    #-------------------------------------------------------------------------
    if do_input_fs_labels:
        labelsNifti = Node(name='labels_nifti',
                           interface=DataGrabber(infields=['subject'],
                                                 outfields=['labels'],
                                                 sort_filelist=False))
        labelsNifti.inputs.base_directory = subjects_dir
        labelsNifti.inputs.template = '%s/mri/' + volume_labels+'.nii.gz'
        labelsNifti.inputs.template_args['labels'] = [['subject']]
        labelsNifti.inputs.subject = subject
    #-------------------------------------------------------------------------
    # Convert FreeSurfer whole-brain label volume to nifti format:
    #-------------------------------------------------------------------------
    else:
        #---------------------------------------------------------------------
        #  label volume:
        #---------------------------------------------------------------------
        labelsMGH = Node(name='labels_mgh',
                         interface=DataGrabber(infields=['subject'],
                                               outfields=['labels'],
                                               sort_filelist=False))
        labelsMGH.inputs.base_directory = subjects_dir
        labelsMGH.inputs.template = '%s/mri/' + volume_labels+'.mgz'
        labelsMGH.inputs.template_args['labels'] = [['subject']]
        labelsMGH.inputs.subject = subject
        #---------------------------------------------------------------------
        # Convert FreeSurfer mgh conformal file to nifti format:
        #---------------------------------------------------------------------
        labelsMGH2Nifti = Node(name='labels_mgh_to_nifti',
                           interface=Fn(function=convert_mgh_to_native_nifti,
                                        input_names=['input_file',
                                                     'reference_file',
                                                     'output_file',
                                                     'interp'],
                                        output_names=['output_file']))
        mbFlow.connect(labelsMGH, 'labels', labelsMGH2Nifti, 'input_file')
        mbFlow.connect(mghOrig, 'mgh_orig', labelsMGH2Nifti, 'reference_file')
        labelsMGH2Nifti.inputs.output_file = ''
        labelsMGH2Nifti.inputs.interp = 'nearest'
        #if save_all:
        #    mbFlow.connect(labelsMGH2Nifti, 'output_file',
        #                   Sink, 'labels.@freesurfer')

    #=========================================================================
    #
    #   Volume labels
    #
    #=========================================================================
    VolLabelFlow = Workflow(name='Volume_labels')

    #---------------------------------------------------------------------
    # Extract FreeSurfer cerebellum labels:
    #---------------------------------------------------------------------
    FScerebellum = Node(name='Extract_FreeSurfer_cerebella',
                        interface=Fn(function=keep_volume_labels,
                                     input_names=['input_file',
                                                  'labels_to_keep',
                                                  'output_file',
                                                  'second_file'],
                                     output_names=['output_file']))
    VolLabelFlow.add_nodes([FScerebellum])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_cerebella.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_cerebella.input_file')
    FScerebellum.inputs.labels_to_keep = dkt.cerebellum_numbers
    FScerebellum.inputs.output_file = ''
    FScerebellum.inputs.second_file = ''

    #=========================================================================
    # Combine FreeSurfer and ANTs cerebrum segmentation volumes
    #=========================================================================
    if not my_segments:
        #---------------------------------------------------------------------
        # Extract FreeSurfer cerebrum labels:
        #---------------------------------------------------------------------
        FScerebrum = Node(name='Extract_FreeSurfer_cerebra',
                          interface=Fn(function=keep_volume_labels,
                                       input_names=['input_file',
                                                    'labels_to_keep',
                                                    'output_file',
                                                    'second_file'],
                                       output_names=['output_file']))
        VolLabelFlow.add_nodes([FScerebrum])
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Extract_FreeSurfer_cerebra.input_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Extract_FreeSurfer_cerebra.input_file')
        labels_to_segment = dkt.cerebrum_cortex_numbers + \
                            dkt.cerebrum_noncortex_numbers + \
                            dkt.brainstem_numbers + \
                            dkt.extra_numbers
        FScerebrum.inputs.labels_to_keep = labels_to_segment
        FScerebrum.inputs.output_file = ''
        FScerebrum.inputs.second_file = ''
        #---------------------------------------------------------------------
        # Convert FreeSurfer cerebrum labels to non/cortex segments:
        #---------------------------------------------------------------------
        FSsegments = Node(name='FreeSurfer_cerebrum_labels_to_segments',
                          interface=Fn(function=relabel_volume,
                                       input_names=['input_file',
                                                    'old_labels',
                                                    'new_labels',
                                                    'output_file'],
                                       output_names=['output_file']))
        VolLabelFlow.add_nodes([FSsegments])
        VolLabelFlow.connect(FScerebrum, 'output_file',
                             FSsegments, 'input_file')
        FSsegments.inputs.old_labels = labels_to_segment
        FSsegments.inputs.new_labels = \
            [2 for x in dkt.cerebrum_cortex_numbers] + \
            [3 for x in dkt.cerebrum_noncortex_numbers] + \
            [3 for x in dkt.brainstem_numbers] + \
            [3 for x in dkt.extra_numbers]
        FSsegments.inputs.output_file = ''
        #---------------------------------------------------------------------
        # Convert ANTs Atropos-segmented volume to non/cortex segments:
        #---------------------------------------------------------------------
        if do_ants:
            ANTsSegments = FSsegments.clone('Relabel_ANTs_segments')
            VolLabelFlow.add_nodes([ANTsSegments])
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'Relabel_ANTs_segments.input_file')
            ANTsSegments.inputs.old_labels = [1, 4]
            ANTsSegments.inputs.new_labels = [0, 3]
            ANTsSegments.inputs.output_file = ''
            #-----------------------------------------------------------------
            # Combine FreeSurfer and ANTs cerebrum segmentation volumes to
            # obtain a single cortex (2) and noncortex (3) segmentation file:
            #-----------------------------------------------------------------
            JoinSegs = Node(name='Combine_FreeSurfer_ANTs_cerebrum_segments',
                            interface=Fn(function=combine_2labels_in_2volumes,
                                         input_names=['file1',
                                                      'file2',
                                                      'label1',
                                                      'label2',
                                                      'output_file'],
                                         output_names=['output_file']))
            VolLabelFlow.add_nodes([JoinSegs])
            VolLabelFlow.connect(FSsegments, 'output_file', JoinSegs, 'file1')
            JoinSegs.inputs.out_dir = ''
            VolLabelFlow.connect(ANTsSegments, 'output_file',
                                 JoinSegs, 'file2')
            JoinSegs.inputs.label1 = 3
            JoinSegs.inputs.label2 = 2
            JoinSegs.inputs.output_file = ''
            #-----------------------------------------------------------------
            # Erase cerebrum that overlaps with FreeSurfer cerebellum:
            #-----------------------------------------------------------------
            if overwrite_cerebrum_with_cerebellum:
                RemoveCerebellum = Node(
                    name='Remove_cerebrum_cerebellum_overlap',
                    interface=Fn(function=remove_volume_labels,
                                 input_names=['input_file',
                                              'labels_to_remove',
                                              'output_file',
                                              'second_file'],
                                 output_names=['output_file']))
                VolLabelFlow.add_nodes([RemoveCerebellum])
                VolLabelFlow.connect(FScerebellum, 'output_file',
                                     RemoveCerebellum, 'input_file')
                RemoveCerebellum.inputs.labels_to_remove = \
                    dkt.cerebellum_numbers
                RemoveCerebellum.inputs.output_file = ''
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     RemoveCerebellum, 'second_file')

    #=========================================================================
    # Split segmented brain into two sides (without medial regions)
    #=========================================================================
    if modify_surface_labels:
        #---------------------------------------------------------------------
        # Split brain by masking with left or right labels:
        #---------------------------------------------------------------------
        SplitBrainSegs = Node(name='Split_brain',
                              interface=Fn(function=split_brain,
                                           input_names=['image_file',
                                                        'label_file',
                                                        'left_labels',
                                                        'right_labels'],
                                           output_names=['left_brain',
                                                         'right_brain']))
        VolLabelFlow.add_nodes([SplitBrainSegs])
        if my_segments:
            SplitBrainSegs.inputs.image_file = my_segments
        else:
            if do_ants:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         SplitBrainSegs, 'image_file')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         SplitBrainSegs, 'image_file')
            else:
                VolLabelFlow.connect(FSsegments, 'output_file',
                                     SplitBrainSegs, 'image_file')
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Split_brain.label_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Split_brain.label_file')
        SplitBrainSegs.inputs.left_labels = dkt.left_cerebrum_numbers
        SplitBrainSegs.inputs.right_labels = dkt.right_cerebrum_numbers

    #=========================================================================
    # Fill cerebrum segmentation volumes with FreeSurfer labels
    #=========================================================================
    #-------------------------------------------------------------------------
    # Extract cerebrum noncortical volume labels:
    #-------------------------------------------------------------------------
    FSnoncortex = Node(name='Extract_FreeSurfer_noncortex_labels',
                       interface=Fn(function=keep_volume_labels,
                                    input_names=['input_file',
                                                 'labels_to_keep',
                                                 'output_file',
                                                 'second_file'],
                                    output_names=['output_file']))
    VolLabelFlow.add_nodes([FSnoncortex])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_noncortex_labels.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_noncortex_labels.input_file')
    labels_to_fill = dkt.cerebrum_noncortex_numbers + \
                     dkt.brainstem_numbers + \
                     dkt.extra_numbers
    FSnoncortex.inputs.labels_to_keep = labels_to_fill
    FSnoncortex.inputs.output_file = ''
    FSnoncortex.inputs.second_file = ''
    #-------------------------------------------------------------------------
    # Propagate FreeSurfer volume labels through noncortex:
    #-------------------------------------------------------------------------
    FSFillNoncortex = Node(name='Fill_noncortex_with_FreeSurfer_labels',
                           interface=Fn(function=PropagateLabelsThroughMask,
                                        input_names=['mask',
                                                     'labels',
                                                     'mask_index',
                                                     'output_file',
                                                     'binarize',
                                                     'stopvalue'],
                                        output_names=['output_file']))
    VolLabelFlow.add_nodes([FSFillNoncortex])
    if my_segments:
        FSFillNoncortex.inputs.mask = my_segments
    else:
        if do_ants:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     FSFillNoncortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     FSFillNoncortex, 'mask')
        else:
            VolLabelFlow.connect(FSsegments, 'output_file',
                                 FSFillNoncortex, 'mask')
    VolLabelFlow.connect(FSnoncortex, 'output_file',
                         FSFillNoncortex, 'labels')
    FSFillNoncortex.inputs.mask_index = 3
    FSFillNoncortex.inputs.output_file = ''
    FSFillNoncortex.inputs.binarize = False
    FSFillNoncortex.inputs.stopvalue = ''
    #-------------------------------------------------------------------------
    # Propagate FreeSurfer surface labels through whole-brain cortex:
    #-------------------------------------------------------------------------
    if do_surface and modify_surface_labels:

        print('NOTE: Reevaluate surface-to-volume label propagation '
              'when surface label modification algorithm complete.')

        #---------------------------------------------------------------------
        # Propagate surface labels through each hemisphere's cortex:
        #---------------------------------------------------------------------
        FSsurfFillCortex = Node(
                    name='Fill_cortex_with_FreeSurfer_surface_labels',
                    interface=Fn(function=fill_volume_with_surface_labels,
                                 input_names=['hemi',
                                              'left_mask',
                                              'right_mask',
                                              'surface_files',
                                              'mask_index',
                                              'output_file',
                                              'binarize'],
                                 output_names=['output_file']))
        VolLabelFlow.add_nodes([FSsurfFillCortex])
        mbFlow.connect(InputHemis, 'hemi', VolLabelFlow,
                       'Fill_cortex_with_FreeSurfer_surface_labels.hemi')
        VolLabelFlow.connect(SplitBrainSegs, 'left_brain',
                             FSsurfFillCortex, 'left_mask')
        VolLabelFlow.connect(SplitBrainSegs, 'right_brain',
                             FSsurfFillCortex, 'right_mask')
        mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
            VolLabelFlow,
            'Fill_cortex_with_FreeSurfer_surface_labels.surface_files')
        FSsurfFillCortex.inputs.mask_index = 2
        FSsurfFillCortex.inputs.output_file = ''
        FSsurfFillCortex.inputs.binarize = False
        #---------------------------------------------------------------------
        # Combine left and right cortical labels:
        #---------------------------------------------------------------------
        SplitHemiList = JoinNode(name='Split_hemisphere_list',
                                 interface=Fn(function=split_list_pair,
                                              input_names=['List'],
                                              output_names=['element1',
                                                            'element2']),
                                 joinsource="Input_hemispheres",
                                 joinfield="List")
        LRcortex = Node(name='Combine_left_right_cortex_labels',
                        interface=Fn(function=ImageMath,
                                     input_names=['volume1',
                                                  'volume2',
                                                  'operator',
                                                  'output_file'],
                                     output_names=['output_file']))
        VolLabelFlow.add_nodes([SplitHemiList, LRcortex])
        VolLabelFlow.connect(FSsurfFillCortex, 'output_file',
                             SplitHemiList, 'List')
        VolLabelFlow.connect(SplitHemiList, 'element1', LRcortex, 'volume1')
        VolLabelFlow.connect(SplitHemiList, 'element2', LRcortex, 'volume2')
        LRcortex.inputs.operator = '+'
        LRcortex.inputs.output_file = ''
    #-------------------------------------------------------------------------
    # Propagate FreeSurfer volume labels through whole-brain cortex:
    #-------------------------------------------------------------------------
    else:
        #---------------------------------------------------------------------
        # Extract FreeSurfer cerebrum cortical volume labels:
        #---------------------------------------------------------------------
        FScortex = FSnoncortex.clone('Extract_FreeSurfer_cortex_labels')
        VolLabelFlow.add_nodes([FScortex])
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Extract_FreeSurfer_cortex_labels.input_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Extract_FreeSurfer_cortex_labels.input_file')
        FScortex.inputs.labels_to_keep = dkt.cerebrum_cortex_numbers
        FScortex.inputs.output_file = ''
        FScortex.inputs.second_file = ''

        #---------------------------------------------------------------------
        # Propagate volume labels through whole-brain cortex:
        #---------------------------------------------------------------------
        FSFillCortex = FSFillNoncortex.clone(
                            'Fill_cortex_with_FreeSurfer_labels')
        VolLabelFlow.add_nodes([FSFillCortex])
        if my_segments:
            FSFillCortex.inputs.mask = my_segments
        else:
            if do_ants:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         FSFillCortex, 'mask')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         FSFillCortex, 'mask')
            else:
                VolLabelFlow.connect(FSsegments, 'output_file',
                                     FSFillCortex, 'mask')
        VolLabelFlow.connect(FScortex, 'output_file',
                             FSFillCortex, 'labels')
        FSFillCortex.inputs.mask_index = 2
    #-------------------------------------------------------------------------
    # Combine FreeSurfer label-filled whole-brain cortex and noncortex:
    #-------------------------------------------------------------------------
    CombineFSLabels = Node(name=
                           'Combine_FreeSurfer_cortex_noncortex_labels',
                           interface=Fn(function=overwrite_volume_labels,
                                        input_names=['source',
                                                     'target',
                                                     'output_file',
                                                     'ignore_labels',
                                                     'erase_labels'],
                                        output_names=['output_file']))
    VolLabelFlow.add_nodes([CombineFSLabels])
    VolLabelFlow.connect(FSFillNoncortex, 'output_file',
                         CombineFSLabels, 'source')
    if modify_surface_labels:
        VolLabelFlow.connect(LRcortex, 'output_file',
                             CombineFSLabels, 'target')
    else:
        VolLabelFlow.connect(FSFillCortex, 'output_file',
                             CombineFSLabels, 'target')
    CombineFSLabels.inputs.output_file = ''
    CombineFSLabels.inputs.ignore_labels = [0]
    CombineFSLabels.inputs.erase_labels = False
    if save_all and not overwrite_cerebrum_with_cerebellum:
        mbFlow.connect(VolLabelFlow,
               'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
               Sink, 'labels.@freesurfer_filled')

    #=========================================================================
    # Fill whole-brain segmentation volumes with ANTs labels
    #=========================================================================
    if do_ants:
        #---------------------------------------------------------------------
        # Mask brain volume:
        #---------------------------------------------------------------------
        #MaskBrain = Node(name= 'Mask_brain',
        #                 interface=Fn(function=ImageMath,
        #                              input_names=['volume1',
        #                                           'volume2',
        #                                           'operator',
        #                                           'output_file'],
        #                              output_names=['output_file']))
        #VolLabelFlow.add_nodes([MaskBrain])
        #mbFlow.connect(MGH2Nifti, 'output_file',
        #               VolLabelFlow, 'Mask_brain.volume1')
        #mbFlow.connect(FetchANTs, 'mask', VolLabelFlow, 'Mask_brain.volume2')
        #MaskBrain.inputs.operator = 'm'
        #MaskBrain.inputs.output_file = ''
        #---------------------------------------------------------------------
        # Transform default atlas labels in MNI152 to subject via template:
        #---------------------------------------------------------------------
        xfm = Node(ApplyTransforms(), name='antsApplyTransforms')
        VolLabelFlow.add_nodes([xfm])
        xfm.inputs.dimension = 3
        xfm.inputs.default_value = 0
        xfm.inputs.interpolation = 'NearestNeighbor'
        xfm.inputs.invert_transform_flags = warp_inverse_Booleans
        xfm.inputs.output_image = 'ANTs_labels.nii.gz'
        if my_segments:
            xfm.inputs.reference_image = my_segments
        else:
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'antsApplyTransforms.reference_image')
        mbFlow.connect(FetchAtlas, 'data_path',
                       VolLabelFlow, 'antsApplyTransforms.input_image')
        mbFlow.connect(WarpToSubjectFileList, 'string_list', VolLabelFlow,
                       'antsApplyTransforms.transforms')
        #if save_all:
        #    mbFlow.connect(VolLabelFlow, 'antsApplyTransforms.output_image',
        #                   Sink, 'labels.@antsRegistration')
        #---------------------------------------------------------------------
        # Extract ANTs cerebral cortical volume labels:
        #---------------------------------------------------------------------
        ANTsCortex = FSnoncortex.clone('Extract_ANTs_cortex_labels')
        VolLabelFlow.add_nodes([ANTsCortex])
        VolLabelFlow.connect(xfm, 'output_image', ANTsCortex, 'input_file')
        ANTsCortex.inputs.labels_to_keep = dkt.cerebrum_cortex_numbers
        ANTsCortex.inputs.output_file = ''
        ANTsCortex.inputs.second_file = ''
        #---------------------------------------------------------------------
        # Extract ANTs whole-brain noncortical volume labels:
        #---------------------------------------------------------------------
        ANTsNoncortex = ANTsCortex.clone('Extract_ANTs_noncortex_labels')
        VolLabelFlow.add_nodes([ANTsNoncortex])
        VolLabelFlow.connect(xfm, 'output_image', ANTsNoncortex, 'input_file')
        ANTsNoncortex.inputs.labels_to_keep = labels_to_fill
        ANTsNoncortex.inputs.output_file = ''
        #---------------------------------------------------------------------
        # Propagate ANTs whole-brain cortical volume labels through cortex:
        #---------------------------------------------------------------------
        ANTsFillCortex = FSFillNoncortex.clone('Fill_cortex_with_ANTs_labels')
        VolLabelFlow.add_nodes([ANTsFillCortex])
        if my_segments:
            ANTsFillCortex.inputs.mask = my_segments
        else:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     ANTsFillCortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     ANTsFillCortex, 'mask')
        VolLabelFlow.connect(ANTsCortex, 'output_file',
                             ANTsFillCortex, 'labels')
        ANTsFillCortex.inputs.mask_index = 2
        ANTsFillCortex.inputs.output_file = ''
        ANTsFillCortex.inputs.binarize = False
        ANTsFillCortex.inputs.stopvalue = ''
        #---------------------------------------------------------------------
        # Propagate ANTs whole-brain noncortical labels through noncortex:
        #---------------------------------------------------------------------
        if fill_noncortex_with_ants_labels:
            ANTsFillNoncortex = ANTsFillCortex.clone(
                'Fill_noncortex_with_ANTs_labels')
            VolLabelFlow.add_nodes([ANTsFillNoncortex])
            if my_segments:
                ANTsFillNoncortex.inputs.mask = my_segments
            else:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         ANTsFillNoncortex, 'mask')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         ANTsFillNoncortex, 'mask')
            VolLabelFlow.connect(ANTsNoncortex, 'output_file',
                                 ANTsFillNoncortex, 'labels')
            ANTsFillNoncortex.inputs.mask_index = 3
        #---------------------------------------------------------------------
        # Combine ANTs label-filled cortex and label-filled noncortex:
        #---------------------------------------------------------------------
        CombineANTsLabels = CombineFSLabels.clone(
            'Combine_ANTs_cortex_noncortex_labels')
        VolLabelFlow.add_nodes([CombineANTsLabels])
        if fill_noncortex_with_ants_labels:
            VolLabelFlow.connect(ANTsFillNoncortex, 'output_file',
                                 CombineANTsLabels, 'source')
        else:
            VolLabelFlow.connect(ANTsNoncortex, 'output_file',
                                 CombineANTsLabels, 'source')
        VolLabelFlow.connect(ANTsFillCortex, 'output_file',
                             CombineANTsLabels, 'target')
        CombineANTsLabels.inputs.output_file = ''
        CombineANTsLabels.inputs.ignore_labels = [0]
        CombineANTsLabels.inputs.erase_labels = False
        if save_all and not overwrite_cerebrum_with_cerebellum:
            mbFlow.connect(VolLabelFlow, 
                           'Combine_ANTs_cortex_noncortex_labels.output_file',
                           Sink, 'labels.@ants_filled')

    #=========================================================================
    # Add FreeSurfer cerebellum labels
    #=========================================================================
    if overwrite_cerebrum_with_cerebellum:
        #---------------------------------------------------------------------
        # ...to FreeSurfer cerebrum labels:
        #---------------------------------------------------------------------
        AddFScerebellum = CombineFSLabels.clone(
            'FreeSurfer_cerebrum_cerebellum')
        VolLabelFlow.add_nodes([AddFScerebellum])
        VolLabelFlow.connect(FScerebellum, 'output_file',
                             AddFScerebellum, 'source')
        VolLabelFlow.connect(CombineFSLabels, 'output_file',
                             AddFScerebellum, 'target')
        AddFScerebellum.inputs.output_file = ''
        AddFScerebellum.inputs.ignore_labels = [0]
        AddFScerebellum.inputs.erase_labels = False
        if save_all:
            mbFlow.connect(VolLabelFlow,
                   'FreeSurfer_cerebrum_cerebellum.output_file',
                    Sink, 'labels.@freesurfer_filled_and_cerebellum')
        #---------------------------------------------------------------------
        # ...to ANTs cerebrum labels:
        #---------------------------------------------------------------------
        if do_ants:
            AddFScerebellum2ANTs = AddFScerebellum.clone(
                'FreeSurfer_cerebellum_ANTs_cerebrum')
            VolLabelFlow.add_nodes([AddFScerebellum2ANTs])
            VolLabelFlow.connect(FScerebellum, 'output_file',
                                 AddFScerebellum2ANTs, 'source')
            VolLabelFlow.connect(CombineANTsLabels, 'output_file',
                                 AddFScerebellum2ANTs, 'target')
            AddFScerebellum2ANTs.inputs.output_file = ''
            AddFScerebellum2ANTs.inputs.ignore_labels = [0]
            AddFScerebellum2ANTs.inputs.erase_labels = False
            if save_all:
                mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       Sink, 'labels.@ants_filled_and_cerebellum')

    #=========================================================================
    # Transform labels from added atlas(es) in MNI152 to subject
    #=========================================================================
    if do_ants and add_atlas_names:
        #---------------------------------------------------------------------
        # Find atlas path that contains atlas name:
        #---------------------------------------------------------------------
        MatchAtlas = Node(name='Match_added_atlas',
                      interface=Fn(function=first_string_containing_substring,
                                   input_names=['substring',
                                                'List'],
                                   output_names=['first_matching_string']))
        VolLabelFlow.add_nodes([MatchAtlas])
        mbFlow.connect(InputAddAtlases, 'atlas', VolLabelFlow,
                       'Match_added_atlas.substring')
        MatchAtlas.inputs.List = atlases
        #---------------------------------------------------------------------
        # Transform atlas:
        #---------------------------------------------------------------------
        xfm2 = xfm.clone('Transform_added_atlases')
        VolLabelFlow.add_nodes([xfm2])
        xfm2.inputs.output_image = 'ANTs_added_atlas_labels.nii.gz'
        VolLabelFlow.connect(MatchAtlas, 'first_matching_string',
                             xfm2, 'input_image')
        if my_segments:
            xfm2.inputs.reference_image = my_segments
        else:
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'Transform_added_atlases.reference_image')
        mbFlow.connect(WarpToSubjectFileList, 'string_list', VolLabelFlow,
                       'Transform_added_atlases.transforms')
        if save_all:
            mbFlow.connect(VolLabelFlow, 'Transform_added_atlases.output_image',
                           Sink, 'labels.@added_atlases')

    ##=========================================================================
    ## Evaluate label volume overlaps
    ##=========================================================================
    #if do_evaluate_vol_labels:
    #
    #    #---------------------------------------------------------------------
    #    # Evaluation inputs: location and structure of atlas volumes
    #    #---------------------------------------------------------------------
    #    VolAtlas = Node(name='Volume_atlas',
    #                    interface=DataGrabber(infields=['subject'],
    #                                          outfields=['atlas_vol_file'],
    #                                          sort_filelist=False))
    #    VolLabelFlow.add_nodes([VolAtlas])
    #    VolAtlas.inputs.base_directory = subjects_dir
    #    VolAtlas.inputs.template = '%s/mri/labels.DKT31.manual.nii.gz'
    #    VolAtlas.inputs.template_args['atlas_vol_file'] = [['subject']]
    #    VolAtlas.inputs.subject = subject
    #    #---------------------------------------------------------------------
    #    # Evaluate volume labels
    #    #---------------------------------------------------------------------
    #    EvalVolLabels = Node(name='Evaluate_volume_labels',
    #                         interface=Fn(function=measure_volume_overlap,
    #                                      input_names=['labels',
    #                                                   'file2',
    #                                                   'file1'],
    #                                      output_names=['overlaps',
    #                                                    'out_file']))
    #    VolLabelFlow.add_nodes([EvalVolLabels])
    #    EvalVolLabels.inputs.labels = dkt.label_numbers
    #    VolLabelFlow.connect(VolAtlas, 'atlas_vol_file',
    #                         EvalVolLabels, 'file2')
    #    if do_ants:
    #        VolLabelFlow.connect(ANTSwFSg, 'output_file',
    #                             EvalVolLabels, 'file1')
    #    else:
    #        VolLabelFlow.connect(CombineFSLabels, 'output_file',
    #                             EvalVolLabels, 'file1')

    #=========================================================================
    #
    #   Volume feature shapes
    #
    #=========================================================================
    if do_shapes:

        VolShapeFlow = Workflow(name='Volume_feature_shapes')

        #=====================================================================
        # Measure volume of each region of a labeled image file
        #=====================================================================
        #---------------------------------------------------------------------
        # Volumes of the FreeSurfer filled labels:
        #---------------------------------------------------------------------
        FSlabelVolumes = Node(name='FreeSurfer_filled_label_volumes',
                              interface=Fn(function=volume_per_label,
                                           input_names=['input_file',
                                                        'include_labels',
                                                        'exclude_labels',
                                                        'label_names',
                                                        'save_table',
                                                        'output_table'],
                                           output_names=['labels_volumes',
                                                         'output_table']))
        VolShapeFlow.add_nodes([FSlabelVolumes])
        FSlabelVolumes.inputs.include_labels = dkt.label_numbers
        FSlabelVolumes.inputs.exclude_labels = []
        FSlabelVolumes.inputs.label_names = dkt.label_names
        FSlabelVolumes.inputs.save_table = True
        FSlabelVolumes.inputs.output_table = 'volumes_FreeSurfer_labels.csv'
        mbFlow.connect(VolLabelFlow,
               'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
               VolShapeFlow, 'FreeSurfer_filled_label_volumes.input_file')
        mbFlow.connect(VolShapeFlow,
                       'FreeSurfer_filled_label_volumes.output_table',
                       Sink, 'tables.@volumes_of_FreeSurfer_labels')
        if do_ants:
            #-----------------------------------------------------------------
            # Volumes of the ANTs filled labels:
            #-----------------------------------------------------------------
            antsLabelVolumes = FSlabelVolumes.clone(
                'ANTs_filled_label_volumes')
            VolShapeFlow.add_nodes([antsLabelVolumes])
            antsLabelVolumes.inputs.exclude_labels = [-1,0]
            antsLabelVolumes.inputs.output_table = 'volumes_ANTs_labels.csv'
            if overwrite_cerebrum_with_cerebellum:
                mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       VolShapeFlow, 'ANTs_filled_label_volumes.input_file')
            else:
                mbFlow.connect(VolLabelFlow,
                       'Combine_ANTs_cortex_noncortex_labels.output_file',
                       VolShapeFlow, 'ANTs_filled_label_volumes.input_file')
            mbFlow.connect(VolShapeFlow,
                           'ANTs_filled_label_volumes.output_table',
                           Sink, 'tables.@volumes_of_ANTs_labels')
            #-----------------------------------------------------------------
            # Volumes of labels in additional atlases transformed to subject:
            #-----------------------------------------------------------------
            if add_atlas_names:
                antsLabelVolumes2 = FSlabelVolumes.clone(
                    'Added_atlas_label_volumes')
                VolShapeFlow.add_nodes([antsLabelVolumes2])
                antsLabelVolumes2.inputs.label_names = []
                antsLabelVolumes2.inputs.exclude_labels = [-1,0]
                mbFlow.connect(VolLabelFlow,
                               'Transform_added_atlases.output_image',
                               VolShapeFlow,
                               'Added_atlas_label_volumes.input_file')
                mbFlow.connect(VolLabelFlow,
                               'Transform_added_atlases.output_image',
                               VolShapeFlow,
                               'Added_atlas_label_volumes.output_table')
                # Save table:
                mbFlow.connect(VolShapeFlow,
                               'Added_atlas_label_volumes.output_table',
                               Sink, 'tables.@volumes_of_added_atlas_labels')

        #=====================================================================
        # Measure volume, thickness of cortical regions of labeled image file
        #=====================================================================
        if args.thickness:
            #-----------------------------------------------------------------
            # Thicknesses of the FreeSurfer cortical labels:
            #-----------------------------------------------------------------
            FSthicknesses = Node(
                name='FreeSurfer_filled_cortex_label_thicknesses',
                interface=Fn(function=thickinthehead,
                             input_names=['segmented_file',
                                          'labeled_file',
                                          'cortex_value',
                                          'noncortex_value',
                                          'labels',
                                          'names',
                                          'resize',
                                          'propagate',
                                          'output_dir',
                                          'save_table',
                                          'output_table'],
                             output_names=['label_volume_thickness',
                                           'output_table']))
            VolShapeFlow.add_nodes([FSthicknesses])
            if my_segments:
                FSthicknesses.inputs.segmented_file = my_segments
            else:
                if do_ants:
                    if overwrite_cerebrum_with_cerebellum:
                        mbFlow.connect(VolLabelFlow,
                            'Remove_cerebrum_cerebellum_overlap.output_file',
                            VolShapeFlow,
                            'FreeSurfer_filled_cortex_label_thicknesses.'
                            'segmented_file')
                    else:
                        mbFlow.connect(VolLabelFlow,
                            'Combine_FreeSurfer_ANTs_cerebrum_segments.'
                            'output_file',
                            VolShapeFlow,
                            'FreeSurfer_filled_cortex_label_thicknesses.'
                            'segmented_file')
                else:
                    mbFlow.connect(VolLabelFlow,
                        'FreeSurfer_cerebrum_labels_to_segments.output_file',
                        VolShapeFlow,
                        'FreeSurfer_filled_cortex_label_thicknesses.'
                        'segmented_file')
            mbFlow.connect(VolLabelFlow,
                'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
                VolShapeFlow,
                'FreeSurfer_filled_cortex_label_thicknesses.labeled_file')
            FSthicknesses.inputs.cortex_value = 2
            FSthicknesses.inputs.noncortex_value = 3
            FSthicknesses.inputs.labels = dkt.cerebrum_cortex_numbers
            FSthicknesses.inputs.names = dkt.cerebrum_cortex_names
            FSthicknesses.inputs.resize = True
            FSthicknesses.inputs.propagate = False
            FSthicknesses.inputs.output_dir = ''
            FSthicknesses.inputs.save_table = True
            FSthicknesses.inputs.output_table = 'thickinthehead_FreeSurfer_labels.csv'
            # Save table:
            mbFlow.connect(VolShapeFlow,
            'FreeSurfer_filled_cortex_label_thicknesses.output_table',
            Sink, 'tables.@thicknesses_of_FreeSurfer_labels')
            #-----------------------------------------------------------------
            # Thicknesses of the ANTs cortical labels:
            #-----------------------------------------------------------------
            if do_ants:
                ANTsThicknesses = FSthicknesses.\
                    clone('ANTs_filled_cortex_label_thicknesses')
                VolShapeFlow.add_nodes([ANTsThicknesses])
                if my_segments:
                    ANTsThicknesses.inputs.segmented_file = my_segments
                else:
                    if overwrite_cerebrum_with_cerebellum:
                        mbFlow.connect(VolLabelFlow,
                            'Remove_cerebrum_cerebellum_overlap.output_file',
                            VolShapeFlow,
                            'ANTs_filled_cortex_label_thicknesses.'
                            'segmented_file')
                    else:
                        mbFlow.connect(VolLabelFlow,
                            'Combine_FreeSurfer_ANTs_cerebrum_segments.'
                            'output_file',
                            VolShapeFlow,
                            'ANTs_filled_cortex_label_thicknesses.'
                            'segmented_file')
                if overwrite_cerebrum_with_cerebellum:
                    mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       VolShapeFlow,
                       'ANTs_filled_cortex_label_thicknesses.labeled_file')
                else:
                    mbFlow.connect(VolLabelFlow,
                    'Combine_ANTs_cortex_noncortex_labels.output_file',
                    VolShapeFlow,
                    'ANTs_filled_cortex_label_thicknesses.labeled_file')
                ANTsThicknesses.inputs.labels = dkt.cerebrum_cortex_numbers_DKT25
                ANTsThicknesses.inputs.names = dkt.cerebrum_cortex_names_DKT25
                ANTsThicknesses.inputs.output_table = 'thickinthehead_ANTs_labels.csv'
                # Save table:
                mbFlow.connect(VolShapeFlow,
                   'ANTs_filled_cortex_label_thicknesses.output_table',
                   Sink, 'tables.@thicknesses_of_ANTs_filled_cortex_labels')


#=============================================================================
#-----------------------------------------------------------------------------
#
#   Run workflows
#
#-----------------------------------------------------------------------------
#=============================================================================
if __name__ == '__main__':

    from time import time
    time0 = time()

    #-------------------------------------------------------------------------
    # Set the workflow configuration enable provenance tracking:
    #-------------------------------------------------------------------------
    #config.enable_provenance()

    #-------------------------------------------------------------------------
    # Set the workflow configuration to use content hashing:
    #-------------------------------------------------------------------------
    mbFlow.config['execution']['hash_method'] = 'content'
    #hashing = args.hashing
    #if hashing:
    #mbFlow.config['execution']['hash_method'] = 'content'
    #mbFlow.config['execution']['use_relative_paths'] = True

    #-------------------------------------------------------------------------
    # Generate a visual graph:
    #-------------------------------------------------------------------------
    graph_vis = args.visual
    if graph_vis == 'hier':
        graph_vis = 'hierarchical'
    if graph_vis:
        if graph_vis == 'exec':
            mbFlow.write_graph(graph2use=graph_vis, simple_form=False)
        else:
            mbFlow.write_graph(graph2use=graph_vis)

    #-------------------------------------------------------------------------
    # Debug: http://nipy.org/nipype/users/config_file.html#debug-configuration
    #-------------------------------------------------------------------------
    debug = False
    if debug:
        config.set('logging', 'workflow_level', 'DEBUG')
        logging.update_logging(config)
        mbFlow.config['execution']['stop_on_first_rerun'] = True
        nproc = 1
    else:
        if args.n:
            nproc = args.n
        else:
            nproc = 1

    #-------------------------------------------------------------------------
    # Run (HTCondor) cluster processes, such as on the Mindboggler cluster:
    #-------------------------------------------------------------------------
    if args.cluster:
        mbFlow.run(plugin='CondorDAGMan')
    #-------------------------------------------------------------------------
    # Run multiple processes or not:
    #-------------------------------------------------------------------------
    else:
        if args.n:
            if nproc > 1:
                mbFlow.run(plugin='MultiProc',
                           plugin_args={'n_procs': args.n})
                           #updatehash=True)
            else:
                mbFlow.run()  #updatehash=True)
        else:
            mbFlow.run()  #updatehash=True)
        # # Default is to use all processors:
        #else:
        #    mbFlow.run(plugin='MultiProc')

    print('Mindboggle run finished ({0:0.2f} seconds).'.format(time() - time0))
